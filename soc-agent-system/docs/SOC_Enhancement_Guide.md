# SOC Agent System - Production Enhancement Guide

This guide adds production-grade features to transform your multi-agent SOC demo into a realistic enterprise system.

---

## Table of Contents

1. [New Features Overview](#new-features-overview)
2. [Enhanced Data Models](#enhanced-data-models)
3. [False Positive Analyzer](#false-positive-analyzer)
4. [Response Action Engine](#response-action-engine)
5. [Investigation Timeline Builder](#investigation-timeline-builder)
6. [Enhanced Coordinator](#enhanced-coordinator)
7. [Frontend Filtering System](#frontend-filtering-system)
8. [Test Script (Replaces UI Triggers)](#test-script)
9. [Updated Components](#updated-components)
10. [Demo Strategy](#demo-strategy)

---

## New Features Overview

### What We're Adding

| Feature | Purpose | Demo Value |
|---------|---------|------------|
| **False Positive Scoring** | Predict likelihood of false positives using historical patterns | Shows ML/analytics capability |
| **Automated Response Actions** | Suggest remediation steps (block, rate limit, escalate) | Shows actionable intelligence |
| **Investigation Timeline** | Chronological event reconstruction | Shows forensic capability |
| **Threat Filtering** | Filter by severity, status, human review required | Shows production UX |
| **Test Script** | CLI tool for generating threats | Removes "demo" feel from UI |

### Architecture After Enhancement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         COORDINATOR AGENT                           â”‚
â”‚                    (Enhanced Orchestration)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚                         â”‚
        â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXISTING    â”‚       â”‚     NEW       â”‚       â”‚     NEW       â”‚
â”‚   5 AGENTS    â”‚       â”‚  FP ANALYZER  â”‚       â”‚   RESPONSE    â”‚
â”‚               â”‚       â”‚               â”‚       â”‚    ENGINE     â”‚
â”‚ â€¢ Historical  â”‚       â”‚ â€¢ Pattern     â”‚       â”‚               â”‚
â”‚ â€¢ Config      â”‚       â”‚   matching    â”‚       â”‚ â€¢ Block IP    â”‚
â”‚ â€¢ DevOps      â”‚       â”‚ â€¢ Confidence  â”‚       â”‚ â€¢ Rate Limit  â”‚
â”‚ â€¢ Context     â”‚       â”‚   scoring     â”‚       â”‚ â€¢ Whitelist   â”‚
â”‚ â€¢ Priority    â”‚       â”‚ â€¢ FP history  â”‚       â”‚ â€¢ Escalate    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                         â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   TIMELINE BUILDER    â”‚
                    â”‚                       â”‚
                    â”‚ â€¢ Event correlation   â”‚
                    â”‚ â€¢ Chronological view  â”‚
                    â”‚ â€¢ Evidence chain      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ENHANCED ANALYSIS   â”‚
                    â”‚                       â”‚
                    â”‚ â€¢ FP Score            â”‚
                    â”‚ â€¢ Response Actions    â”‚
                    â”‚ â€¢ Investigation TL    â”‚
                    â”‚ â€¢ MITRE Mapping       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Enhanced Data Models

**File:** `backend/src/models.py`

Replace or extend your existing models file:

```python
"""Enhanced data models for SOC Agent System."""
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Any
from pydantic import BaseModel, Field
import uuid


# ============================================================================
# ENUMS
# ============================================================================

class ThreatType(str, Enum):
    """Types of threats the system can detect."""
    BOT_TRAFFIC = "bot_traffic"
    CREDENTIAL_STUFFING = "credential_stuffing"
    ACCOUNT_TAKEOVER = "account_takeover"
    RATE_LIMIT_BREACH = "rate_limit_breach"
    GEO_ANOMALY = "geo_anomaly"
    API_ABUSE = "api_abuse"
    DATA_SCRAPING = "data_scraping"
    BRUTE_FORCE = "brute_force"


class ThreatSeverity(str, Enum):
    """Severity levels for threats."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class ThreatStatus(str, Enum):
    """Status of threat analysis."""
    PENDING = "pending"
    ANALYZING = "analyzing"
    COMPLETED = "completed"
    ESCALATED = "escalated"
    RESOLVED = "resolved"
    FALSE_POSITIVE = "false_positive"


class ResponseActionType(str, Enum):
    """Types of automated response actions."""
    BLOCK_IP = "block_ip"
    RATE_LIMIT = "rate_limit"
    CHALLENGE = "challenge"
    WHITELIST = "whitelist"
    MONITOR = "monitor"
    ESCALATE = "escalate"
    QUARANTINE = "quarantine"
    NONE = "none"


class ResponseUrgency(str, Enum):
    """Urgency level for response actions."""
    IMMEDIATE = "immediate"
    URGENT = "urgent"
    NORMAL = "normal"
    LOW = "low"


class TimelineEventType(str, Enum):
    """Types of events in investigation timeline."""
    DETECTION = "detection"
    ENRICHMENT = "enrichment"
    ANALYSIS = "analysis"
    CORRELATION = "correlation"
    DECISION = "decision"
    ACTION = "action"
    ESCALATION = "escalation"


class MITRETactic(str, Enum):
    """MITRE ATT&CK Tactics."""
    INITIAL_ACCESS = "Initial Access"
    EXECUTION = "Execution"
    PERSISTENCE = "Persistence"
    PRIVILEGE_ESCALATION = "Privilege Escalation"
    DEFENSE_EVASION = "Defense Evasion"
    CREDENTIAL_ACCESS = "Credential Access"
    DISCOVERY = "Discovery"
    LATERAL_MOVEMENT = "Lateral Movement"
    COLLECTION = "Collection"
    EXFILTRATION = "Exfiltration"
    IMPACT = "Impact"


class MITRETechnique(str, Enum):
    """MITRE ATT&CK Techniques (subset)."""
    VALID_ACCOUNTS = "T1078 - Valid Accounts"
    BRUTE_FORCE = "T1110 - Brute Force"
    CREDENTIAL_STUFFING = "T1110.004 - Credential Stuffing"
    AUTOMATED_COLLECTION = "T1119 - Automated Collection"
    DATA_FROM_INFO_REPOS = "T1213 - Data from Information Repositories"
    WEB_SERVICE = "T1102 - Web Service"
    APPLICATION_LAYER_PROTOCOL = "T1071 - Application Layer Protocol"


# ============================================================================
# CORE MODELS
# ============================================================================

class ThreatSignal(BaseModel):
    """Raw threat signal from detection systems."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    threat_type: ThreatType
    customer_name: str
    customer_id: str = Field(default_factory=lambda: str(uuid.uuid4())[:8])
    source_ip: str = "0.0.0.0"
    user_agent: Optional[str] = None
    request_count: int = 0
    time_window_minutes: int = 5
    detected_at: datetime = Field(default_factory=datetime.utcnow)
    raw_data: Dict[str, Any] = Field(default_factory=dict)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


class AgentAnalysis(BaseModel):
    """Analysis result from a single agent."""
    agent_name: str
    analysis: str
    confidence: float = Field(ge=0.0, le=1.0)
    key_findings: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    processing_time_ms: int = 0
    data_sources_consulted: List[str] = Field(default_factory=list)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


# ============================================================================
# NEW: FALSE POSITIVE SCORING
# ============================================================================

class FalsePositiveIndicator(BaseModel):
    """Individual indicator contributing to FP score."""
    indicator: str
    weight: float = Field(ge=-1.0, le=1.0)  # Negative = likely real threat
    description: str
    source: str  # Which agent/analyzer identified this


class FalsePositiveScore(BaseModel):
    """Comprehensive false positive analysis."""
    score: float = Field(ge=0.0, le=1.0)  # 0 = definitely real, 1 = definitely FP
    confidence: float = Field(ge=0.0, le=1.0)
    indicators: List[FalsePositiveIndicator] = Field(default_factory=list)
    historical_fp_rate: Optional[float] = None  # FP rate for similar threats
    similar_resolved_as_fp: int = 0  # Count of similar threats resolved as FP
    similar_resolved_as_real: int = 0  # Count of similar threats confirmed real
    recommendation: str = ""  # "likely_false_positive", "likely_real_threat", "needs_review"
    explanation: str = ""


# ============================================================================
# NEW: RESPONSE ACTIONS
# ============================================================================

class ResponseAction(BaseModel):
    """A recommended or executed response action."""
    action_type: ResponseActionType
    urgency: ResponseUrgency
    target: str  # IP, user, endpoint, etc.
    reason: str
    confidence: float = Field(ge=0.0, le=1.0)
    auto_executable: bool = False  # Can be auto-executed without human approval
    requires_approval: bool = True
    estimated_impact: str = ""  # "Low", "Medium", "High"
    rollback_possible: bool = True
    parameters: Dict[str, Any] = Field(default_factory=dict)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


class ResponsePlan(BaseModel):
    """Complete response plan with multiple actions."""
    primary_action: ResponseAction
    secondary_actions: List[ResponseAction] = Field(default_factory=list)
    escalation_path: List[str] = Field(default_factory=list)  # Team/person escalation order
    sla_minutes: int = 60  # Time to respond
    auto_escalate_after_minutes: int = 30
    notes: str = ""


# ============================================================================
# NEW: INVESTIGATION TIMELINE
# ============================================================================

class TimelineEvent(BaseModel):
    """Single event in the investigation timeline."""
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    event_type: TimelineEventType
    title: str
    description: str
    source: str  # Agent name, system, or data source
    data: Dict[str, Any] = Field(default_factory=dict)
    severity: Optional[ThreatSeverity] = None

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


class InvestigationTimeline(BaseModel):
    """Complete investigation timeline."""
    events: List[TimelineEvent] = Field(default_factory=list)
    start_time: datetime = Field(default_factory=datetime.utcnow)
    end_time: Optional[datetime] = None
    duration_ms: int = 0

    def add_event(self, event: TimelineEvent):
        self.events.append(event)
        self.events.sort(key=lambda e: e.timestamp)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


# ============================================================================
# ENHANCED THREAT ANALYSIS
# ============================================================================

class ThreatAnalysis(BaseModel):
    """Complete threat analysis with all enhancements."""
    # Core fields
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    signal: ThreatSignal
    status: ThreatStatus = ThreatStatus.COMPLETED
    severity: ThreatSeverity

    # Analysis results
    executive_summary: str
    customer_narrative: str = ""
    agent_analyses: Dict[str, AgentAnalysis] = Field(default_factory=dict)

    # MITRE mapping
    mitre_tactics: List[MITRETactic] = Field(default_factory=list)
    mitre_techniques: List[MITRETechnique] = Field(default_factory=list)

    # NEW: False positive scoring
    false_positive_score: Optional[FalsePositiveScore] = None

    # NEW: Response actions
    response_plan: Optional[ResponsePlan] = None

    # NEW: Investigation timeline
    investigation_timeline: Optional[InvestigationTimeline] = None

    # Metadata
    requires_human_review: bool = False
    review_reason: Optional[str] = None
    total_processing_time_ms: int = 0
    analyzed_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


# ============================================================================
# FILTERING & DASHBOARD
# ============================================================================

class ThreatFilter(BaseModel):
    """Filter criteria for threat queries."""
    severities: Optional[List[ThreatSeverity]] = None
    statuses: Optional[List[ThreatStatus]] = None
    threat_types: Optional[List[ThreatType]] = None
    requires_human_review: Optional[bool] = None
    customer_name: Optional[str] = None
    min_fp_score: Optional[float] = None
    max_fp_score: Optional[float] = None
    from_date: Optional[datetime] = None
    to_date: Optional[datetime] = None
    search_query: Optional[str] = None


class DashboardMetrics(BaseModel):
    """Metrics for the dashboard."""
    total_threats: int = 0
    threats_by_severity: Dict[str, int] = Field(default_factory=dict)
    threats_by_status: Dict[str, int] = Field(default_factory=dict)
    threats_by_type: Dict[str, int] = Field(default_factory=dict)
    pending_human_review: int = 0
    avg_processing_time_ms: float = 0
    false_positive_rate: float = 0
    threats_last_hour: int = 0
    threats_last_24h: int = 0


# ============================================================================
# HISTORICAL DATA MODELS
# ============================================================================

class HistoricalIncident(BaseModel):
    """Historical incident for pattern matching."""
    id: str
    threat_type: ThreatType
    customer_name: str
    severity: ThreatSeverity
    resolved_as: str  # "true_positive", "false_positive", "inconclusive"
    resolution_notes: str = ""
    detected_at: datetime
    resolved_at: Optional[datetime] = None
    indicators: List[str] = Field(default_factory=list)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


class CustomerConfig(BaseModel):
    """Customer-specific configuration."""
    customer_name: str
    customer_id: str
    tier: str = "standard"  # "standard", "premium", "enterprise"
    rate_limit_rpm: int = 1000
    bot_protection_enabled: bool = True
    auto_block_enabled: bool = False
    escalation_contacts: List[str] = Field(default_factory=list)
    whitelist_ips: List[str] = Field(default_factory=list)
    custom_rules: Dict[str, Any] = Field(default_factory=dict)


class InfraEvent(BaseModel):
    """Infrastructure event for correlation."""
    id: str
    event_type: str  # "deployment", "config_change", "incident", "maintenance"
    description: str
    timestamp: datetime
    affected_services: List[str] = Field(default_factory=list)
    severity: str = "info"

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}


class NewsItem(BaseModel):
    """External news/intelligence item."""
    id: str
    title: str
    summary: str
    source: str
    url: Optional[str] = None
    published_at: datetime
    relevance_score: float = 0.5
    keywords: List[str] = Field(default_factory=list)

    class Config:
        json_encoders = {datetime: lambda v: v.isoformat()}
```

---

## False Positive Analyzer

**File:** `backend/src/analyzers/fp_analyzer.py` (new file)

```python
"""False Positive Analyzer - Predicts likelihood of false positives."""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from models import (
    ThreatSignal, FalsePositiveScore, FalsePositiveIndicator,
    ThreatType, HistoricalIncident
)

logger = logging.getLogger(__name__)


class FalsePositiveAnalyzer:
    """Analyzes threat signals to predict false positive likelihood."""

    # Historical FP rates by threat type (mock data - would be from DB in production)
    BASELINE_FP_RATES = {
        ThreatType.BOT_TRAFFIC: 0.35,  # 35% FP rate
        ThreatType.CREDENTIAL_STUFFING: 0.15,
        ThreatType.ACCOUNT_TAKEOVER: 0.10,
        ThreatType.RATE_LIMIT_BREACH: 0.45,
        ThreatType.GEO_ANOMALY: 0.55,
        ThreatType.API_ABUSE: 0.25,
        ThreatType.DATA_SCRAPING: 0.40,
        ThreatType.BRUTE_FORCE: 0.20,
    }

    # Known benign patterns
    BENIGN_USER_AGENTS = [
        "googlebot", "bingbot", "slackbot", "facebookexternalhit",
        "twitterbot", "linkedinbot", "pingdom", "uptimerobot"
    ]

    BENIGN_IP_PATTERNS = [
        "66.249.",  # Google
        "157.55.",  # Microsoft/Bing
        "40.77.",   # Microsoft
    ]

    def __init__(self, historical_incidents: Optional[List[HistoricalIncident]] = None):
        """Initialize with optional historical data."""
        self.historical_incidents = historical_incidents or []

    def analyze(
        self, 
        signal: ThreatSignal, 
        agent_analyses: Dict[str, Any],
        similar_incidents: List[HistoricalIncident]
    ) -> FalsePositiveScore:
        """
        Analyze a threat signal and predict FP likelihood.

        Args:
            signal: The threat signal to analyze
            agent_analyses: Results from all agents
            similar_incidents: Historical incidents similar to this one

        Returns:
            FalsePositiveScore with prediction and indicators
        """
        logger.info(f"ðŸ” Analyzing FP likelihood for {signal.threat_type.value}")

        indicators: List[FalsePositiveIndicator] = []

        # 1. Check user agent patterns
        ua_indicator = self._analyze_user_agent(signal)
        if ua_indicator:
            indicators.append(ua_indicator)

        # 2. Check IP patterns
        ip_indicator = self._analyze_ip(signal)
        if ip_indicator:
            indicators.append(ip_indicator)

        # 3. Check request volume patterns
        volume_indicator = self._analyze_request_volume(signal)
        if volume_indicator:
            indicators.append(volume_indicator)

        # 4. Analyze historical patterns
        history_indicators = self._analyze_historical_patterns(
            signal, similar_incidents
        )
        indicators.extend(history_indicators)

        # 5. Check agent confidence levels
        confidence_indicator = self._analyze_agent_confidence(agent_analyses)
        if confidence_indicator:
            indicators.append(confidence_indicator)

        # 6. Check for known benign patterns
        benign_indicator = self._check_benign_patterns(signal)
        if benign_indicator:
            indicators.append(benign_indicator)

        # Calculate final score
        fp_score = self._calculate_score(signal, indicators, similar_incidents)

        logger.info(f"   FP Score: {fp_score.score:.2f} ({fp_score.recommendation})")

        return fp_score

    def _analyze_user_agent(self, signal: ThreatSignal) -> Optional[FalsePositiveIndicator]:
        """Check if user agent suggests benign traffic."""
        if not signal.user_agent:
            return None

        ua_lower = signal.user_agent.lower()

        for benign_ua in self.BENIGN_USER_AGENTS:
            if benign_ua in ua_lower:
                return FalsePositiveIndicator(
                    indicator=f"Known benign bot: {benign_ua}",
                    weight=0.4,  # Strong FP indicator
                    description=f"User agent matches known benign crawler: {benign_ua}",
                    source="FP Analyzer - User Agent Check"
                )

        # Check for suspicious patterns
        suspicious_patterns = ["python-requests", "curl", "wget", "scanner"]
        for pattern in suspicious_patterns:
            if pattern in ua_lower:
                return FalsePositiveIndicator(
                    indicator=f"Suspicious user agent: {pattern}",
                    weight=-0.2,  # Slight real threat indicator
                    description=f"User agent contains suspicious pattern: {pattern}",
                    source="FP Analyzer - User Agent Check"
                )

        return None

    def _analyze_ip(self, signal: ThreatSignal) -> Optional[FalsePositiveIndicator]:
        """Check if IP suggests benign or malicious traffic."""
        for benign_prefix in self.BENIGN_IP_PATTERNS:
            if signal.source_ip.startswith(benign_prefix):
                return FalsePositiveIndicator(
                    indicator=f"Known benign IP range: {benign_prefix}*",
                    weight=0.5,  # Strong FP indicator
                    description=f"IP belongs to known benign service provider",
                    source="FP Analyzer - IP Check"
                )

        return None

    def _analyze_request_volume(self, signal: ThreatSignal) -> Optional[FalsePositiveIndicator]:
        """Analyze request volume patterns."""
        rpm = signal.request_count / max(signal.time_window_minutes, 1)

        if rpm < 10:
            return FalsePositiveIndicator(
                indicator="Low request volume",
                weight=0.2,
                description=f"Only {rpm:.1f} requests/minute - may be normal traffic",
                source="FP Analyzer - Volume Check"
            )
        elif rpm > 1000:
            return FalsePositiveIndicator(
                indicator="Extremely high request volume",
                weight=-0.3,
                description=f"{rpm:.0f} requests/minute indicates automated attack",
                source="FP Analyzer - Volume Check"
            )

        return None

    def _analyze_historical_patterns(
        self, 
        signal: ThreatSignal,
        similar_incidents: List[HistoricalIncident]
    ) -> List[FalsePositiveIndicator]:
        """Analyze historical incident patterns."""
        indicators = []

        if not similar_incidents:
            return indicators

        # Count resolution types
        fp_count = sum(1 for i in similar_incidents if i.resolved_as == "false_positive")
        tp_count = sum(1 for i in similar_incidents if i.resolved_as == "true_positive")
        total = len(similar_incidents)

        if total > 0:
            fp_rate = fp_count / total

            if fp_rate > 0.5:
                indicators.append(FalsePositiveIndicator(
                    indicator=f"High historical FP rate: {fp_rate:.0%}",
                    weight=0.3,
                    description=f"{fp_count}/{total} similar incidents were false positives",
                    source="FP Analyzer - Historical Analysis"
                ))
            elif fp_rate < 0.2:
                indicators.append(FalsePositiveIndicator(
                    indicator=f"Low historical FP rate: {fp_rate:.0%}",
                    weight=-0.3,
                    description=f"Only {fp_count}/{total} similar incidents were false positives",
                    source="FP Analyzer - Historical Analysis"
                ))

        # Check for recurring customer patterns
        customer_incidents = [i for i in similar_incidents 
                            if i.customer_name == signal.customer_name]
        if len(customer_incidents) >= 3:
            customer_fp = sum(1 for i in customer_incidents 
                            if i.resolved_as == "false_positive")
            if customer_fp >= 2:
                indicators.append(FalsePositiveIndicator(
                    indicator="Recurring FP pattern for customer",
                    weight=0.25,
                    description=f"Customer has {customer_fp} previous false positives",
                    source="FP Analyzer - Customer History"
                ))

        return indicators

    def _analyze_agent_confidence(
        self, 
        agent_analyses: Dict[str, Any]
    ) -> Optional[FalsePositiveIndicator]:
        """Check agent confidence levels for FP signals."""
        if not agent_analyses:
            return None

        confidences = []
        for name, analysis in agent_analyses.items():
            if hasattr(analysis, 'confidence'):
                confidences.append(analysis.confidence)

        if not confidences:
            return None

        avg_confidence = sum(confidences) / len(confidences)

        if avg_confidence < 0.5:
            return FalsePositiveIndicator(
                indicator="Low agent confidence",
                weight=0.2,
                description=f"Average agent confidence is {avg_confidence:.0%}",
                source="FP Analyzer - Agent Confidence"
            )
        elif avg_confidence > 0.85:
            return FalsePositiveIndicator(
                indicator="High agent confidence",
                weight=-0.2,
                description=f"Average agent confidence is {avg_confidence:.0%}",
                source="FP Analyzer - Agent Confidence"
            )

        return None

    def _check_benign_patterns(self, signal: ThreatSignal) -> Optional[FalsePositiveIndicator]:
        """Check for known benign patterns in raw data."""
        raw_data = signal.raw_data or {}

        # Check for monitoring/health check patterns
        if raw_data.get("endpoint", "").lower() in ["/health", "/ping", "/status", "/ready"]:
            return FalsePositiveIndicator(
                indicator="Health check endpoint",
                weight=0.4,
                description="Traffic to health check endpoint is typically benign",
                source="FP Analyzer - Endpoint Check"
            )

        # Check for known internal IPs
        if signal.source_ip.startswith("10.") or signal.source_ip.startswith("192.168."):
            return FalsePositiveIndicator(
                indicator="Internal IP address",
                weight=0.3,
                description="Traffic from internal network",
                source="FP Analyzer - IP Check"
            )

        return None

    def _calculate_score(
        self,
        signal: ThreatSignal,
        indicators: List[FalsePositiveIndicator],
        similar_incidents: List[HistoricalIncident]
    ) -> FalsePositiveScore:
        """Calculate final FP score from all indicators."""

        # Start with baseline rate for this threat type
        baseline = self.BASELINE_FP_RATES.get(signal.threat_type, 0.3)

        # Apply indicator weights
        total_weight = sum(i.weight for i in indicators)
        adjusted_score = baseline + (total_weight * 0.3)  # Scale factor

        # Clamp to 0-1
        final_score = max(0.0, min(1.0, adjusted_score))

        # Calculate confidence based on data quality
        confidence = 0.5  # Base confidence
        if similar_incidents:
            confidence += min(0.3, len(similar_incidents) * 0.05)
        if indicators:
            confidence += min(0.2, len(indicators) * 0.04)
        confidence = min(1.0, confidence)

        # Determine recommendation
        if final_score >= 0.7:
            recommendation = "likely_false_positive"
            explanation = "Multiple indicators suggest this is likely a false positive. Consider auto-dismissing or quick review."
        elif final_score >= 0.4:
            recommendation = "needs_review"
            explanation = "Mixed signals - human review recommended to confirm threat status."
        else:
            recommendation = "likely_real_threat"
            explanation = "Strong indicators of real threat. Prioritize investigation and response."

        # Count historical resolutions
        fp_count = sum(1 for i in similar_incidents if i.resolved_as == "false_positive")
        tp_count = sum(1 for i in similar_incidents if i.resolved_as == "true_positive")

        return FalsePositiveScore(
            score=round(final_score, 3),
            confidence=round(confidence, 3),
            indicators=indicators,
            historical_fp_rate=self.BASELINE_FP_RATES.get(signal.threat_type),
            similar_resolved_as_fp=fp_count,
            similar_resolved_as_real=tp_count,
            recommendation=recommendation,
            explanation=explanation
        )
```

---

## Response Action Engine

**File:** `backend/src/analyzers/response_engine.py` (new file)

```python
"""Response Action Engine - Determines appropriate response actions."""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from models import (
    ThreatSignal, ThreatSeverity, ThreatType, FalsePositiveScore,
    ResponseAction, ResponsePlan, ResponseActionType, ResponseUrgency,
    CustomerConfig, AgentAnalysis
)

logger = logging.getLogger(__name__)


class ResponseActionEngine:
    """Determines appropriate automated response actions for threats."""

    # Response templates by threat type and severity
    RESPONSE_TEMPLATES = {
        ThreatType.BOT_TRAFFIC: {
            ThreatSeverity.CRITICAL: [
                (ResponseActionType.BLOCK_IP, ResponseUrgency.IMMEDIATE),
                (ResponseActionType.RATE_LIMIT, ResponseUrgency.IMMEDIATE),
            ],
            ThreatSeverity.HIGH: [
                (ResponseActionType.RATE_LIMIT, ResponseUrgency.URGENT),
                (ResponseActionType.CHALLENGE, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.MEDIUM: [
                (ResponseActionType.CHALLENGE, ResponseUrgency.NORMAL),
                (ResponseActionType.MONITOR, ResponseUrgency.NORMAL),
            ],
            ThreatSeverity.LOW: [
                (ResponseActionType.MONITOR, ResponseUrgency.LOW),
            ],
        },
        ThreatType.CREDENTIAL_STUFFING: {
            ThreatSeverity.CRITICAL: [
                (ResponseActionType.BLOCK_IP, ResponseUrgency.IMMEDIATE),
                (ResponseActionType.ESCALATE, ResponseUrgency.IMMEDIATE),
            ],
            ThreatSeverity.HIGH: [
                (ResponseActionType.BLOCK_IP, ResponseUrgency.URGENT),
                (ResponseActionType.CHALLENGE, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.MEDIUM: [
                (ResponseActionType.CHALLENGE, ResponseUrgency.NORMAL),
                (ResponseActionType.RATE_LIMIT, ResponseUrgency.NORMAL),
            ],
            ThreatSeverity.LOW: [
                (ResponseActionType.MONITOR, ResponseUrgency.NORMAL),
            ],
        },
        ThreatType.ACCOUNT_TAKEOVER: {
            ThreatSeverity.CRITICAL: [
                (ResponseActionType.QUARANTINE, ResponseUrgency.IMMEDIATE),
                (ResponseActionType.ESCALATE, ResponseUrgency.IMMEDIATE),
                (ResponseActionType.BLOCK_IP, ResponseUrgency.IMMEDIATE),
            ],
            ThreatSeverity.HIGH: [
                (ResponseActionType.QUARANTINE, ResponseUrgency.URGENT),
                (ResponseActionType.CHALLENGE, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.MEDIUM: [
                (ResponseActionType.CHALLENGE, ResponseUrgency.NORMAL),
                (ResponseActionType.MONITOR, ResponseUrgency.NORMAL),
            ],
            ThreatSeverity.LOW: [
                (ResponseActionType.MONITOR, ResponseUrgency.LOW),
            ],
        },
        ThreatType.RATE_LIMIT_BREACH: {
            ThreatSeverity.CRITICAL: [
                (ResponseActionType.RATE_LIMIT, ResponseUrgency.IMMEDIATE),
                (ResponseActionType.BLOCK_IP, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.HIGH: [
                (ResponseActionType.RATE_LIMIT, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.MEDIUM: [
                (ResponseActionType.RATE_LIMIT, ResponseUrgency.NORMAL),
            ],
            ThreatSeverity.LOW: [
                (ResponseActionType.MONITOR, ResponseUrgency.LOW),
            ],
        },
        ThreatType.GEO_ANOMALY: {
            ThreatSeverity.CRITICAL: [
                (ResponseActionType.CHALLENGE, ResponseUrgency.IMMEDIATE),
                (ResponseActionType.ESCALATE, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.HIGH: [
                (ResponseActionType.CHALLENGE, ResponseUrgency.URGENT),
            ],
            ThreatSeverity.MEDIUM: [
                (ResponseActionType.CHALLENGE, ResponseUrgency.NORMAL),
            ],
            ThreatSeverity.LOW: [
                (ResponseActionType.MONITOR, ResponseUrgency.LOW),
            ],
        },
    }

    # SLA times by severity (minutes)
    SLA_TIMES = {
        ThreatSeverity.CRITICAL: 15,
        ThreatSeverity.HIGH: 30,
        ThreatSeverity.MEDIUM: 60,
        ThreatSeverity.LOW: 240,
        ThreatSeverity.INFO: 480,
    }

    def __init__(self):
        """Initialize response engine."""
        pass

    def generate_response_plan(
        self,
        signal: ThreatSignal,
        severity: ThreatSeverity,
        fp_score: Optional[FalsePositiveScore],
        customer_config: Optional[CustomerConfig],
        agent_analyses: Dict[str, AgentAnalysis]
    ) -> ResponsePlan:
        """
        Generate a response plan for the threat.

        Args:
            signal: The threat signal
            severity: Assessed severity
            fp_score: False positive analysis
            customer_config: Customer-specific settings
            agent_analyses: Results from all agents

        Returns:
            Complete ResponsePlan with actions
        """
        logger.info(f"âš¡ Generating response plan for {severity.value} {signal.threat_type.value}")

        # Check if likely false positive - recommend minimal action
        if fp_score and fp_score.score >= 0.7:
            logger.info(f"   High FP score ({fp_score.score:.2f}) - recommending minimal action")
            return self._generate_fp_response_plan(signal, fp_score)

        # Get response templates for this threat type and severity
        templates = self.RESPONSE_TEMPLATES.get(signal.threat_type, {})
        actions = templates.get(severity, [(ResponseActionType.MONITOR, ResponseUrgency.NORMAL)])

        # Build response actions
        response_actions = []
        for action_type, urgency in actions:
            action = self._build_action(
                action_type=action_type,
                urgency=urgency,
                signal=signal,
                severity=severity,
                customer_config=customer_config
            )
            response_actions.append(action)

        # Check customer config for auto-block settings
        if customer_config and customer_config.auto_block_enabled:
            for action in response_actions:
                if action.action_type == ResponseActionType.BLOCK_IP:
                    action.auto_executable = True
                    action.requires_approval = False

        # Primary action is the first (highest priority)
        primary_action = response_actions[0] if response_actions else self._build_monitor_action(signal)
        secondary_actions = response_actions[1:] if len(response_actions) > 1 else []

        # Build escalation path
        escalation_path = self._build_escalation_path(severity, customer_config)

        # Determine SLA
        sla_minutes = self.SLA_TIMES.get(severity, 60)

        return ResponsePlan(
            primary_action=primary_action,
            secondary_actions=secondary_actions,
            escalation_path=escalation_path,
            sla_minutes=sla_minutes,
            auto_escalate_after_minutes=sla_minutes // 2,
            notes=self._generate_response_notes(signal, severity, agent_analyses)
        )

    def _generate_fp_response_plan(
        self, 
        signal: ThreatSignal, 
        fp_score: FalsePositiveScore
    ) -> ResponsePlan:
        """Generate minimal response plan for likely false positives."""
        return ResponsePlan(
            primary_action=ResponseAction(
                action_type=ResponseActionType.MONITOR,
                urgency=ResponseUrgency.LOW,
                target=signal.source_ip,
                reason=f"Likely false positive (score: {fp_score.score:.2f})",
                confidence=fp_score.confidence,
                auto_executable=True,
                requires_approval=False,
                estimated_impact="Low",
                rollback_possible=True,
                parameters={"duration_minutes": 30}
            ),
            secondary_actions=[],
            escalation_path=["SOC Tier 1"],
            sla_minutes=240,
            auto_escalate_after_minutes=120,
            notes=f"High FP likelihood. {fp_score.explanation}"
        )

    def _build_action(
        self,
        action_type: ResponseActionType,
        urgency: ResponseUrgency,
        signal: ThreatSignal,
        severity: ThreatSeverity,
        customer_config: Optional[CustomerConfig]
    ) -> ResponseAction:
        """Build a specific response action."""

        # Action-specific configurations
        action_configs = {
            ResponseActionType.BLOCK_IP: {
                "reason": f"Block malicious IP due to {signal.threat_type.value}",
                "impact": "High" if severity in [ThreatSeverity.CRITICAL, ThreatSeverity.HIGH] else "Medium",
                "params": {"duration_minutes": 60, "scope": "customer"},
                "auto_exec": False,
            },
            ResponseActionType.RATE_LIMIT: {
                "reason": f"Apply rate limiting due to {signal.threat_type.value}",
                "impact": "Medium",
                "params": {"requests_per_minute": 10, "duration_minutes": 30},
                "auto_exec": True,
            },
            ResponseActionType.CHALLENGE: {
                "reason": f"Require CAPTCHA/challenge due to {signal.threat_type.value}",
                "impact": "Low",
                "params": {"challenge_type": "captcha", "duration_minutes": 60},
                "auto_exec": True,
            },
            ResponseActionType.WHITELIST: {
                "reason": "Add to whitelist - confirmed legitimate traffic",
                "impact": "Low",
                "params": {"duration_minutes": 1440},
                "auto_exec": False,
            },
            ResponseActionType.MONITOR: {
                "reason": f"Enhanced monitoring for {signal.threat_type.value}",
                "impact": "Low",
                "params": {"duration_minutes": 60, "alert_threshold": 100},
                "auto_exec": True,
            },
            ResponseActionType.ESCALATE: {
                "reason": f"Escalate {severity.value} {signal.threat_type.value} for review",
                "impact": "Low",
                "params": {"escalation_level": "Tier 2"},
                "auto_exec": True,
            },
            ResponseActionType.QUARANTINE: {
                "reason": f"Quarantine affected account due to {signal.threat_type.value}",
                "impact": "High",
                "params": {"notify_user": True},
                "auto_exec": False,
            },
        }

        config = action_configs.get(action_type, {
            "reason": f"Respond to {signal.threat_type.value}",
            "impact": "Medium",
            "params": {},
            "auto_exec": False,
        })

        # Determine target based on action type
        if action_type in [ResponseActionType.BLOCK_IP, ResponseActionType.RATE_LIMIT, 
                          ResponseActionType.CHALLENGE, ResponseActionType.MONITOR]:
            target = signal.source_ip
        elif action_type == ResponseActionType.QUARANTINE:
            target = signal.raw_data.get("user_id", signal.customer_name)
        else:
            target = signal.customer_name

        return ResponseAction(
            action_type=action_type,
            urgency=urgency,
            target=target,
            reason=config["reason"],
            confidence=0.8 if severity in [ThreatSeverity.CRITICAL, ThreatSeverity.HIGH] else 0.6,
            auto_executable=config["auto_exec"],
            requires_approval=not config["auto_exec"],
            estimated_impact=config["impact"],
            rollback_possible=True,
            parameters=config["params"]
        )

    def _build_monitor_action(self, signal: ThreatSignal) -> ResponseAction:
        """Build a default monitor action."""
        return ResponseAction(
            action_type=ResponseActionType.MONITOR,
            urgency=ResponseUrgency.NORMAL,
            target=signal.source_ip,
            reason="Standard monitoring",
            confidence=0.5,
            auto_executable=True,
            requires_approval=False,
            estimated_impact="Low",
            rollback_possible=True,
            parameters={"duration_minutes": 60}
        )

    def _build_escalation_path(
        self, 
        severity: ThreatSeverity,
        customer_config: Optional[CustomerConfig]
    ) -> List[str]:
        """Build escalation path based on severity and customer config."""
        base_path = {
            ThreatSeverity.CRITICAL: ["SOC Tier 2", "SOC Manager", "CISO", "Customer Success"],
            ThreatSeverity.HIGH: ["SOC Tier 2", "SOC Manager", "Customer Success"],
            ThreatSeverity.MEDIUM: ["SOC Tier 1", "SOC Tier 2"],
            ThreatSeverity.LOW: ["SOC Tier 1"],
            ThreatSeverity.INFO: ["SOC Tier 1"],
        }

        path = base_path.get(severity, ["SOC Tier 1"])

        # Add customer-specific contacts if available
        if customer_config and customer_config.escalation_contacts:
            path.extend(customer_config.escalation_contacts[:2])

        return path

    def _generate_response_notes(
        self,
        signal: ThreatSignal,
        severity: ThreatSeverity,
        agent_analyses: Dict[str, AgentAnalysis]
    ) -> str:
        """Generate notes summarizing response rationale."""
        notes = []

        notes.append(f"Threat: {signal.threat_type.value.replace('_', ' ').title()}")
        notes.append(f"Severity: {severity.value.upper()}")
        notes.append(f"Customer: {signal.customer_name}")

        # Add key agent insights
        for name, analysis in agent_analyses.items():
            if analysis.key_findings:
                notes.append(f"{name.title()}: {analysis.key_findings[0]}")

        return " | ".join(notes)
```

---

## Investigation Timeline Builder

**File:** `backend/src/analyzers/timeline_builder.py` (new file)

```python
"""Investigation Timeline Builder - Creates chronological event timeline."""
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import random

from models import (
    ThreatSignal, InvestigationTimeline, TimelineEvent, TimelineEventType,
    ThreatSeverity, AgentAnalysis, FalsePositiveScore, ResponsePlan
)

logger = logging.getLogger(__name__)


class TimelineBuilder:
    """Builds investigation timelines for threat analysis."""

    def __init__(self):
        """Initialize timeline builder."""
        self.events: List[TimelineEvent] = []
        self.start_time: Optional[datetime] = None

    def build_timeline(
        self,
        signal: ThreatSignal,
        agent_analyses: Dict[str, AgentAnalysis],
        fp_score: Optional[FalsePositiveScore],
        response_plan: Optional[ResponsePlan],
        severity: ThreatSeverity
    ) -> InvestigationTimeline:
        """
        Build complete investigation timeline.

        Args:
            signal: Original threat signal
            agent_analyses: Results from all agents
            fp_score: False positive analysis
            response_plan: Generated response plan
            severity: Assessed severity

        Returns:
            Complete InvestigationTimeline
        """
        logger.info("ðŸ“… Building investigation timeline...")

        self.events = []
        self.start_time = signal.detected_at

        # 1. Detection event
        self._add_detection_event(signal)

        # 2. Enrichment events (data gathering)
        self._add_enrichment_events(signal)

        # 3. Agent analysis events
        self._add_agent_analysis_events(agent_analyses)

        # 4. FP analysis event
        if fp_score:
            self._add_fp_analysis_event(fp_score)

        # 5. Correlation events
        self._add_correlation_events(signal, agent_analyses)

        # 6. Decision event (severity determination)
        self._add_decision_event(severity, fp_score)

        # 7. Response action events
        if response_plan:
            self._add_response_events(response_plan)

        # Calculate duration
        end_time = datetime.utcnow()
        duration_ms = int((end_time - self.start_time).total_seconds() * 1000)

        timeline = InvestigationTimeline(
            events=sorted(self.events, key=lambda e: e.timestamp),
            start_time=self.start_time,
            end_time=end_time,
            duration_ms=duration_ms
        )

        logger.info(f"   Timeline built with {len(self.events)} events")

        return timeline

    def _add_detection_event(self, signal: ThreatSignal):
        """Add initial detection event."""
        self.events.append(TimelineEvent(
            timestamp=signal.detected_at,
            event_type=TimelineEventType.DETECTION,
            title="Threat Detected",
            description=f"{signal.threat_type.value.replace('_', ' ').title()} detected from {signal.source_ip}",
            source="Detection Engine",
            data={
                "threat_type": signal.threat_type.value,
                "source_ip": signal.source_ip,
                "customer": signal.customer_name,
                "request_count": signal.request_count,
            },
            severity=ThreatSeverity.INFO
        ))

    def _add_enrichment_events(self, signal: ThreatSignal):
        """Add data enrichment events."""
        base_time = signal.detected_at + timedelta(milliseconds=50)

        # Historical data lookup
        self.events.append(TimelineEvent(
            timestamp=base_time,
            event_type=TimelineEventType.ENRICHMENT,
            title="Historical Data Retrieved",
            description="Queried incident database for similar past events",
            source="Historical Database",
            data={"query_type": "similar_incidents", "time_range": "90 days"}
        ))

        # Customer config lookup
        self.events.append(TimelineEvent(
            timestamp=base_time + timedelta(milliseconds=20),
            event_type=TimelineEventType.ENRICHMENT,
            title="Customer Configuration Loaded",
            description=f"Retrieved security settings for {signal.customer_name}",
            source="Config Service",
            data={"customer": signal.customer_name}
        ))

        # Infrastructure events lookup
        self.events.append(TimelineEvent(
            timestamp=base_time + timedelta(milliseconds=35),
            event_type=TimelineEventType.ENRICHMENT,
            title="Infrastructure Events Retrieved",
            description="Queried recent deployments and infrastructure changes",
            source="DevOps Platform",
            data={"time_range": "60 minutes"}
        ))

        # Threat intelligence lookup
        self.events.append(TimelineEvent(
            timestamp=base_time + timedelta(milliseconds=50),
            event_type=TimelineEventType.ENRICHMENT,
            title="Threat Intelligence Gathered",
            description="Retrieved relevant security news and bulletins",
            source="Threat Intel Feed",
            data={"keywords": [signal.customer_name, signal.threat_type.value]}
        ))

    def _add_agent_analysis_events(self, agent_analyses: Dict[str, AgentAnalysis]):
        """Add agent analysis events."""
        base_time = self.start_time + timedelta(milliseconds=100)

        agent_descriptions = {
            "historical": "Analyzed patterns from similar past incidents",
            "config": "Evaluated against customer security policies",
            "devops": "Correlated with infrastructure events",
            "context": "Assessed business context and external factors",
            "priority": "Determined severity and classification"
        }

        for name, analysis in agent_analyses.items():
            # Add small random offset to show parallel execution
            offset = timedelta(milliseconds=random.randint(0, 50))

            self.events.append(TimelineEvent(
                timestamp=base_time + offset,
                event_type=TimelineEventType.ANALYSIS,
                title=f"{name.title()} Agent Analysis",
                description=agent_descriptions.get(name, "Performed specialized analysis"),
                source=f"{name.title()} Agent",
                data={
                    "confidence": analysis.confidence,
                    "key_findings": analysis.key_findings[:2],
                    "processing_time_ms": analysis.processing_time_ms
                }
            ))

    def _add_fp_analysis_event(self, fp_score: FalsePositiveScore):
        """Add false positive analysis event."""
        self.events.append(TimelineEvent(
            timestamp=self.start_time + timedelta(milliseconds=800),
            event_type=TimelineEventType.ANALYSIS,
            title="False Positive Analysis",
            description=f"FP likelihood assessed: {fp_score.recommendation.replace('_', ' ')}",
            source="FP Analyzer",
            data={
                "fp_score": fp_score.score,
                "confidence": fp_score.confidence,
                "indicators_count": len(fp_score.indicators),
                "recommendation": fp_score.recommendation
            }
        ))

    def _add_correlation_events(
        self, 
        signal: ThreatSignal,
        agent_analyses: Dict[str, AgentAnalysis]
    ):
        """Add correlation events."""
        base_time = self.start_time + timedelta(milliseconds=850)

        # Cross-agent correlation
        self.events.append(TimelineEvent(
            timestamp=base_time,
            event_type=TimelineEventType.CORRELATION,
            title="Cross-Agent Correlation",
            description="Synthesized findings from all specialized agents",
            source="Coordinator",
            data={
                "agents_correlated": list(agent_analyses.keys()),
                "consensus_reached": True
            }
        ))

        # If historical patterns found
        historical = agent_analyses.get("historical")
        if historical and historical.key_findings:
            self.events.append(TimelineEvent(
                timestamp=base_time + timedelta(milliseconds=20),
                event_type=TimelineEventType.CORRELATION,
                title="Historical Pattern Match",
                description="Correlated with similar past incidents",
                source="Historical Agent",
                data={"pattern_match": historical.key_findings[0] if historical.key_findings else "None"}
            ))

    def _add_decision_event(
        self, 
        severity: ThreatSeverity,
        fp_score: Optional[FalsePositiveScore]
    ):
        """Add decision event for severity determination."""
        decision_data = {
            "severity": severity.value,
            "severity_factors": []
        }

        if fp_score:
            decision_data["fp_score"] = fp_score.score
            decision_data["fp_recommendation"] = fp_score.recommendation

        description = f"Threat classified as {severity.value.upper()} severity"
        if fp_score and fp_score.score >= 0.7:
            description += " (likely false positive)"

        self.events.append(TimelineEvent(
            timestamp=self.start_time + timedelta(milliseconds=900),
            event_type=TimelineEventType.DECISION,
            title="Severity Classification",
            description=description,
            source="Coordinator",
            data=decision_data,
            severity=severity
        ))

    def _add_response_events(self, response_plan: ResponsePlan):
        """Add response action events."""
        base_time = self.start_time + timedelta(milliseconds=950)

        # Primary action recommendation
        primary = response_plan.primary_action
        self.events.append(TimelineEvent(
            timestamp=base_time,
            event_type=TimelineEventType.ACTION,
            title=f"Primary Response: {primary.action_type.value.replace('_', ' ').title()}",
            description=primary.reason,
            source="Response Engine",
            data={
                "action_type": primary.action_type.value,
                "target": primary.target,
                "urgency": primary.urgency.value,
                "auto_executable": primary.auto_executable,
                "requires_approval": primary.requires_approval
            }
        ))

        # Secondary actions
        for i, action in enumerate(response_plan.secondary_actions):
            self.events.append(TimelineEvent(
                timestamp=base_time + timedelta(milliseconds=10 * (i + 1)),
                event_type=TimelineEventType.ACTION,
                title=f"Secondary Response: {action.action_type.value.replace('_', ' ').title()}",
                description=action.reason,
                source="Response Engine",
                data={
                    "action_type": action.action_type.value,
                    "target": action.target,
                    "urgency": action.urgency.value
                }
            ))

        # Escalation if needed
        if response_plan.escalation_path:
            self.events.append(TimelineEvent(
                timestamp=base_time + timedelta(milliseconds=50),
                event_type=TimelineEventType.ESCALATION,
                title="Escalation Path Set",
                description=f"Auto-escalate after {response_plan.auto_escalate_after_minutes} minutes if unresolved",
                source="Response Engine",
                data={
                    "escalation_path": response_plan.escalation_path,
                    "sla_minutes": response_plan.sla_minutes
                }
            ))
```

---

## Enhanced Coordinator

**File:** `backend/src/agents/coordinator.py`

Update the coordinator to integrate all new analyzers:

```python
"""Enhanced Coordinator Agent with FP scoring, response actions, and timeline."""
import asyncio
import time
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
from openai import AsyncOpenAI

from models import (
    ThreatSignal, ThreatAnalysis, ThreatSeverity, ThreatStatus,
    AgentAnalysis, MITRETactic, MITRETechnique, CustomerConfig
)
from config import settings
from mock_data import MockDataStore
from agents.historical_agent import HistoricalAgent
from agents.config_agent import ConfigAgent
from agents.devops_agent import DevOpsAgent
from agents.context_agent import ContextAgent
from agents.priority_agent import PriorityAgent
from analyzers.fp_analyzer import FalsePositiveAnalyzer
from analyzers.response_engine import ResponseActionEngine
from analyzers.timeline_builder import TimelineBuilder

logger = logging.getLogger(__name__)


class CoordinatorAgent:
    """Enhanced orchestrator with FP scoring, response actions, and timeline."""

    def __init__(
        self, 
        mock_data: Optional[MockDataStore] = None,
        client: Optional[AsyncOpenAI] = None,
        use_mock: bool = False
    ):
        """Initialize coordinator with all specialized agents and analyzers."""
        self.client = client or AsyncOpenAI(api_key=settings.openai_api_key)
        self.mock_data = mock_data or MockDataStore()
        self.use_mock = use_mock

        # Initialize specialized agents
        self.historical_agent = HistoricalAgent(client=self.client)
        self.config_agent = ConfigAgent(client=self.client)
        self.devops_agent = DevOpsAgent(client=self.client)
        self.context_agent = ContextAgent(client=self.client)
        self.priority_agent = PriorityAgent(client=self.client)

        # Initialize new analyzers
        self.fp_analyzer = FalsePositiveAnalyzer()
        self.response_engine = ResponseActionEngine()
        self.timeline_builder = TimelineBuilder()

        logger.info("ðŸŽ¯ Enhanced Coordinator initialized")
        logger.info("   â”œâ”€ 5 Specialized Agents")
        logger.info("   â”œâ”€ False Positive Analyzer")
        logger.info("   â”œâ”€ Response Action Engine")
        logger.info("   â””â”€ Timeline Builder")

    async def analyze_threat(self, signal: ThreatSignal) -> ThreatAnalysis:
        """Perform comprehensive threat analysis with all enhancements."""
        logger.info("=" * 80)
        logger.info(f"ðŸš¨ NEW THREAT ANALYSIS: {signal.threat_type.value}")
        logger.info(f"   Customer: {signal.customer_name}")
        logger.info(f"   Source IP: {signal.source_ip}")
        logger.info(f"   Signal ID: {signal.id}")
        logger.info("=" * 80)

        start_time = time.time()

        # Phase 1: Gather context
        logger.info("\nðŸ“Š PHASE 1: GATHERING CONTEXT...")
        contexts = self._build_agent_contexts(signal)
        similar_incidents = contexts["historical"].get("similar_incidents", [])
        customer_config = contexts["config"].get("customer_config")

        logger.info(f"   âœ“ Historical: {len(similar_incidents)} similar incidents")
        logger.info(f"   âœ“ Config: {customer_config.tier if customer_config else 'default'} tier customer")
        logger.info(f"   âœ“ DevOps: {len(contexts['devops'].get('infra_events', []))} infra events")
        logger.info(f"   âœ“ Context: {len(contexts['context'].get('news_items', []))} news items")

        # Phase 2: Parallel agent analysis
        logger.info("\nðŸ¤– PHASE 2: PARALLEL AGENT ANALYSIS...")
        analyze_method = "analyze_mock" if self.use_mock else "analyze"
        mode = "MOCK" if self.use_mock else "LIVE"

        dispatch_start = time.time()
        results = await asyncio.gather(
            self._log_agent_execution("Historical", getattr(self.historical_agent, analyze_method), signal, contexts["historical"]),
            self._log_agent_execution("Config", getattr(self.config_agent, analyze_method), signal, contexts["config"]),
            self._log_agent_execution("DevOps", getattr(self.devops_agent, analyze_method), signal, contexts["devops"]),
            self._log_agent_execution("Context", getattr(self.context_agent, analyze_method), signal, contexts["context"]),
            self._log_agent_execution("Priority", getattr(self.priority_agent, analyze_method), signal, contexts["priority"]),
            return_exceptions=True
        )

        dispatch_time = (time.time() - dispatch_start) * 1000
        logger.info(f"\n   âš¡ All agents completed in {dispatch_time:.0f}ms ({mode})")

        # Process results
        agent_analyses = self._process_agent_results(results)

        # Determine initial severity
        severity = self._determine_severity(agent_analyses)
        logger.info(f"\n   ðŸ“Š Initial severity assessment: {severity.value}")

        # Phase 3: False positive analysis
        logger.info("\nðŸ” PHASE 3: FALSE POSITIVE ANALYSIS...")
        fp_score = self.fp_analyzer.analyze(signal, agent_analyses, similar_incidents)
        logger.info(f"   FP Score: {fp_score.score:.2f} ({fp_score.recommendation})")
        logger.info(f"   Confidence: {fp_score.confidence:.2f}")

        # Phase 4: Response action planning
        logger.info("\nâš¡ PHASE 4: RESPONSE PLANNING...")
        response_plan = self.response_engine.generate_response_plan(
            signal=signal,
            severity=severity,
            fp_score=fp_score,
            customer_config=customer_config,
            agent_analyses=agent_analyses
        )
        logger.info(f"   Primary Action: {response_plan.primary_action.action_type.value}")
        logger.info(f"   Urgency: {response_plan.primary_action.urgency.value}")
        logger.info(f"   SLA: {response_plan.sla_minutes} minutes")

        # Phase 5: Build investigation timeline
        logger.info("\nðŸ“… PHASE 5: BUILDING TIMELINE...")
        timeline = self.timeline_builder.build_timeline(
            signal=signal,
            agent_analyses=agent_analyses,
            fp_score=fp_score,
            response_plan=response_plan,
            severity=severity
        )
        logger.info(f"   Timeline events: {len(timeline.events)}")

        # Phase 6: Final synthesis
        logger.info("\nðŸ”¬ PHASE 6: FINAL SYNTHESIS...")
        total_time = int((time.time() - start_time) * 1000)

        # Determine if human review needed
        requires_review, review_reason = self._check_human_review_required(
            severity, fp_score, response_plan
        )

        # Generate MITRE mapping
        mitre_tactics, mitre_techniques = self._map_mitre(signal.threat_type)

        # Generate executive summary
        executive_summary = self._generate_executive_summary(
            signal, severity, agent_analyses, fp_score
        )

        # Generate customer narrative
        customer_narrative = self._generate_customer_narrative(
            signal, severity, fp_score, response_plan
        )

        analysis = ThreatAnalysis(
            signal=signal,
            status=ThreatStatus.COMPLETED,
            severity=severity,
            executive_summary=executive_summary,
            customer_narrative=customer_narrative,
            agent_analyses=agent_analyses,
            mitre_tactics=mitre_tactics,
            mitre_techniques=mitre_techniques,
            false_positive_score=fp_score,
            response_plan=response_plan,
            investigation_timeline=timeline,
            requires_human_review=requires_review,
            review_reason=review_reason,
            total_processing_time_ms=total_time
        )

        logger.info("\nâœ… ANALYSIS COMPLETE")
        logger.info(f"   Severity: {severity.value}")
        logger.info(f"   FP Score: {fp_score.score:.2f}")
        logger.info(f"   Primary Action: {response_plan.primary_action.action_type.value}")
        logger.info(f"   Human Review: {requires_review}")
        logger.info(f"   Total Time: {total_time}ms")
        logger.info("=" * 80 + "\n")

        return analysis

    async def _log_agent_execution(
        self,
        agent_name: str,
        analyze_func,
        signal: ThreatSignal,
        context: Dict[str, Any]
    ) -> AgentAnalysis:
        """Wrapper to log individual agent execution."""
        logger.info(f"   ðŸ”„ {agent_name} Agent starting...")
        start = time.time()

        try:
            result = await analyze_func(signal, context)
            elapsed = (time.time() - start) * 1000
            logger.info(f"   âœ… {agent_name} completed ({elapsed:.0f}ms, conf: {result.confidence:.2f})")
            return result
        except Exception as e:
            elapsed = (time.time() - start) * 1000
            logger.error(f"   âŒ {agent_name} failed ({elapsed:.0f}ms): {str(e)}")
            raise

    def _build_agent_contexts(self, signal: ThreatSignal) -> Dict[str, Dict[str, Any]]:
        """Build context data for each agent."""
        keywords = [signal.customer_name, signal.threat_type.value]

        similar_incidents = self.mock_data.get_similar_incidents(
            signal.threat_type, signal.customer_name
        )
        customer_config = self.mock_data.get_customer_config(signal.customer_name)
        infra_events = self.mock_data.get_recent_infra_events(60)
        news_items = self.mock_data.get_relevant_news(keywords)

        return {
            "historical": {"similar_incidents": similar_incidents},
            "config": {"customer_config": customer_config},
            "devops": {"infra_events": infra_events},
            "context": {"news_items": news_items},
            "priority": {}
        }

    def _process_agent_results(self, results: List[Any]) -> Dict[str, AgentAnalysis]:
        """Process agent results, handling errors."""
        agent_names = ["historical", "config", "devops", "context", "priority"]
        agent_analyses = {}

        for name, result in zip(agent_names, results):
            if isinstance(result, Exception):
                logger.error(f"   âŒ {name} agent failed: {str(result)}")
                agent_analyses[name] = AgentAnalysis(
                    agent_name=name,
                    analysis=f"Agent failed: {str(result)}",
                    confidence=0.0,
                    key_findings=["Error"],
                    recommendations=["Manual review required"],
                    processing_time_ms=0
                )
            else:
                agent_analyses[name] = result

        return agent_analyses

    def _determine_severity(self, agent_analyses: Dict[str, AgentAnalysis]) -> ThreatSeverity:
        """Determine threat severity from agent analyses."""
        priority_analysis = agent_analyses.get("priority")

        if priority_analysis and priority_analysis.confidence > 0:
            analysis_lower = priority_analysis.analysis.lower()
            if "critical" in analysis_lower:
                return ThreatSeverity.CRITICAL
            elif "high" in analysis_lower:
                return ThreatSeverity.HIGH
            elif "low" in analysis_lower:
                return ThreatSeverity.LOW

        return ThreatSeverity.MEDIUM

    def _check_human_review_required(
        self,
        severity: ThreatSeverity,
        fp_score: Any,
        response_plan: Any
    ) -> tuple:
        """Check if human review is required."""
        reasons = []

        if severity == ThreatSeverity.CRITICAL:
            reasons.append("Critical severity requires human verification")

        if fp_score and 0.3 <= fp_score.score <= 0.7:
            reasons.append("Ambiguous false positive score requires review")

        if response_plan and response_plan.primary_action.requires_approval:
            reasons.append(f"Action '{response_plan.primary_action.action_type.value}' requires approval")

        requires_review = len(reasons) > 0
        review_reason = "; ".join(reasons) if reasons else None

        return requires_review, review_reason

    def _map_mitre(self, threat_type: Any) -> tuple:
        """Map threat type to MITRE ATT&CK."""
        mitre_mapping = {
            "bot_traffic": (
                [MITRETactic.INITIAL_ACCESS],
                [MITRETechnique.APPLICATION_LAYER_PROTOCOL]
            ),
            "credential_stuffing": (
                [MITRETactic.CREDENTIAL_ACCESS],
                [MITRETechnique.CREDENTIAL_STUFFING, MITRETechnique.BRUTE_FORCE]
            ),
            "account_takeover": (
                [MITRETactic.CREDENTIAL_ACCESS, MITRETactic.PERSISTENCE],
                [MITRETechnique.VALID_ACCOUNTS]
            ),
            "data_scraping": (
                [MITRETactic.COLLECTION],
                [MITRETechnique.AUTOMATED_COLLECTION, MITRETechnique.DATA_FROM_INFO_REPOS]
            ),
            "brute_force": (
                [MITRETactic.CREDENTIAL_ACCESS],
                [MITRETechnique.BRUTE_FORCE]
            ),
        }

        return mitre_mapping.get(
            threat_type.value if hasattr(threat_type, 'value') else threat_type,
            ([], [])
        )

    def _generate_executive_summary(
        self,
        signal: ThreatSignal,
        severity: ThreatSeverity,
        agent_analyses: Dict[str, AgentAnalysis],
        fp_score: Any
    ) -> str:
        """Generate executive summary."""
        all_findings = []
        for analysis in agent_analyses.values():
            all_findings.extend(analysis.key_findings[:2])

        findings_text = "; ".join(all_findings[:3]) if all_findings else "Analysis completed"

        fp_note = ""
        if fp_score and fp_score.score >= 0.7:
            fp_note = " (Likely false positive)"
        elif fp_score and fp_score.score <= 0.3:
            fp_note = " (High confidence threat)"

        return (
            f"{severity.value.upper()} severity {signal.threat_type.value.replace('_', ' ')} "
            f"detected for {signal.customer_name}{fp_note}. "
            f"Key findings: {findings_text}."
        )

    def _generate_customer_narrative(
        self,
        signal: ThreatSignal,
        severity: ThreatSeverity,
        fp_score: Any,
        response_plan: Any
    ) -> str:
        """Generate customer-facing narrative."""
        if fp_score and fp_score.score >= 0.7:
            return (
                f"We detected unusual traffic patterns from your infrastructure that initially "
                f"appeared suspicious. After analysis, our system determined this is likely "
                f"legitimate traffic. No action is required from your team."
            )

        action_text = response_plan.primary_action.action_type.value.replace('_', ' ') if response_plan else "monitoring"

        return (
            f"We detected {signal.threat_type.value.replace('_', ' ')} activity targeting your "
            f"infrastructure. This has been classified as {severity.value} severity. "
            f"Our recommended response is to {action_text}. "
            f"Please review the detailed analysis and recommended actions."
        )


def create_coordinator(use_mock: bool = False) -> CoordinatorAgent:
    """Factory function for coordinator."""
    return CoordinatorAgent(use_mock=use_mock)
```

---

## Frontend Filtering System

### Filter Component

**File:** `frontend/src/components/ThreatFilters.jsx` (new file)

```jsx
import React, { useState } from 'react';

const ThreatFilters = ({ filters, onFilterChange, metrics }) => {
  const [isExpanded, setIsExpanded] = useState(false);

  const severityOptions = [
    { value: 'critical', label: 'Critical', color: 'bg-red-500' },
    { value: 'high', label: 'High', color: 'bg-orange-500' },
    { value: 'medium', label: 'Medium', color: 'bg-yellow-500' },
    { value: 'low', label: 'Low', color: 'bg-blue-500' },
  ];

  const statusOptions = [
    { value: 'pending', label: 'Pending' },
    { value: 'analyzing', label: 'Analyzing' },
    { value: 'completed', label: 'Completed' },
    { value: 'escalated', label: 'Escalated' },
    { value: 'resolved', label: 'Resolved' },
    { value: 'false_positive', label: 'False Positive' },
  ];

  const handleSeverityToggle = (severity) => {
    const current = filters.severities || [];
    const updated = current.includes(severity)
      ? current.filter(s => s !== severity)
      : [...current, severity];
    onFilterChange({ ...filters, severities: updated.length ? updated : null });
  };

  const handleStatusToggle = (status) => {
    const current = filters.statuses || [];
    const updated = current.includes(status)
      ? current.filter(s => s !== status)
      : [...current, status];
    onFilterChange({ ...filters, statuses: updated.length ? updated : null });
  };

  const handleReviewToggle = () => {
    onFilterChange({
      ...filters,
      requires_human_review: filters.requires_human_review === true ? null : true
    });
  };

  const handleSearchChange = (e) => {
    onFilterChange({ ...filters, search_query: e.target.value || null });
  };

  const clearFilters = () => {
    onFilterChange({
      severities: null,
      statuses: null,
      requires_human_review: null,
      search_query: null,
    });
  };

  const activeFilterCount = [
    filters.severities?.length > 0,
    filters.statuses?.length > 0,
    filters.requires_human_review === true,
    filters.search_query?.length > 0,
  ].filter(Boolean).length;

  return (
    <div className="bg-gray-800 rounded-lg p-4 mb-4">
      {/* Header Row */}
      <div className="flex items-center justify-between mb-3">
        <div className="flex items-center gap-4">
          <h3 className="text-white font-semibold">Filters</h3>

          {/* Quick filter: Human Review Required */}
          <button
            onClick={handleReviewToggle}
            className={`px-3 py-1.5 rounded-lg text-sm font-medium transition-colors flex items-center gap-2 ${
              filters.requires_human_review === true
                ? 'bg-amber-500 text-white'
                : 'bg-gray-700 text-gray-300 hover:bg-gray-600'
            }`}
          >
            <svg className="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} 
                d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z" 
              />
            </svg>
            Needs Review
            {metrics?.pending_human_review > 0 && (
              <span className="bg-amber-600 px-1.5 py-0.5 rounded text-xs">
                {metrics.pending_human_review}
              </span>
            )}
          </button>

          {/* Search */}
          <div className="relative">
            <input
              type="text"
              placeholder="Search threats..."
              value={filters.search_query || ''}
              onChange={handleSearchChange}
              className="bg-gray-700 text-white pl-8 pr-3 py-1.5 rounded-lg text-sm w-64 focus:outline-none focus:ring-2 focus:ring-blue-500"
            />
            <svg className="w-4 h-4 text-gray-400 absolute left-2.5 top-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />
            </svg>
          </div>
        </div>

        <div className="flex items-center gap-2">
          {activeFilterCount > 0 && (
            <button
              onClick={clearFilters}
              className="text-gray-400 hover:text-white text-sm flex items-center gap-1"
            >
              <svg className="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
              </svg>
              Clear ({activeFilterCount})
            </button>
          )}

          <button
            onClick={() => setIsExpanded(!isExpanded)}
            className="text-gray-400 hover:text-white text-sm flex items-center gap-1"
          >
            {isExpanded ? 'Less' : 'More'}
            <svg className={`w-4 h-4 transition-transform ${isExpanded ? 'rotate-180' : ''}`} fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 9l-7 7-7-7" />
            </svg>
          </button>
        </div>
      </div>

      {/* Severity Quick Filters */}
      <div className="flex gap-2 mb-3">
        {severityOptions.map(({ value, label, color }) => (
          <button
            key={value}
            onClick={() => handleSeverityToggle(value)}
            className={`px-3 py-1 rounded text-sm font-medium transition-colors ${
              (filters.severities || []).includes(value)
                ? `${color} text-white`
                : 'bg-gray-700 text-gray-300 hover:bg-gray-600'
            }`}
          >
            {label}
            {metrics?.threats_by_severity?.[value] > 0 && (
              <span className="ml-1 opacity-75">({metrics.threats_by_severity[value]})</span>
            )}
          </button>
        ))}
      </div>

      {/* Expanded Filters */}
      {isExpanded && (
        <div className="pt-3 border-t border-gray-700">
          <div className="mb-3">
            <label className="text-gray-400 text-sm mb-2 block">Status</label>
            <div className="flex flex-wrap gap-2">
              {statusOptions.map(({ value, label }) => (
                <button
                  key={value}
                  onClick={() => handleStatusToggle(value)}
                  className={`px-3 py-1 rounded text-sm transition-colors ${
                    (filters.statuses || []).includes(value)
                      ? 'bg-blue-500 text-white'
                      : 'bg-gray-700 text-gray-300 hover:bg-gray-600'
                  }`}
                >
                  {label}
                </button>
              ))}
            </div>
          </div>
        </div>
      )}
    </div>
  );
};

export default ThreatFilters;
```

### Updated Threat List

**File:** `frontend/src/components/ThreatList.jsx`

Add filtering logic to your existing ThreatList component:

```jsx
import React, { useState, useMemo } from 'react';
import ThreatFilters from './ThreatFilters';
import ThreatCard from './ThreatCard';

const ThreatList = ({ threats, metrics }) => {
  const [filters, setFilters] = useState({
    severities: null,
    statuses: null,
    requires_human_review: null,
    search_query: null,
  });

  // Apply filters
  const filteredThreats = useMemo(() => {
    return threats.filter(threat => {
      // Severity filter
      if (filters.severities?.length > 0) {
        if (!filters.severities.includes(threat.severity)) return false;
      }

      // Status filter
      if (filters.statuses?.length > 0) {
        if (!filters.statuses.includes(threat.status)) return false;
      }

      // Human review filter
      if (filters.requires_human_review === true) {
        if (!threat.requires_human_review) return false;
      }

      // Search filter
      if (filters.search_query) {
        const query = filters.search_query.toLowerCase();
        const searchable = [
          threat.signal?.customer_name,
          threat.signal?.threat_type,
          threat.signal?.source_ip,
          threat.executive_summary,
        ].filter(Boolean).join(' ').toLowerCase();

        if (!searchable.includes(query)) return false;
      }

      return true;
    });
  }, [threats, filters]);

  return (
    <div>
      <ThreatFilters
        filters={filters}
        onFilterChange={setFilters}
        metrics={metrics}
      />

      {/* Results count */}
      <div className="text-gray-400 text-sm mb-3">
        Showing {filteredThreats.length} of {threats.length} threats
      </div>

      {/* Threat cards */}
      <div className="space-y-4">
        {filteredThreats.length === 0 ? (
          <div className="text-center py-12 text-gray-500">
            <svg className="w-12 h-12 mx-auto mb-3 opacity-50" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z" />
            </svg>
            <p>No threats match your filters</p>
          </div>
        ) : (
          filteredThreats.map(threat => (
            <ThreatCard key={threat.id} threat={threat} />
          ))
        )}
      </div>
    </div>
  );
};

export default ThreatList;
```

---

## Test Script

**File:** `backend/test_threats.py` (new file)

This replaces the UI trigger buttons with a professional test script:

```python
#!/usr/bin/env python3
"""
SOC Agent System - Threat Test Script

This script generates test threats for the SOC Agent System.
Use instead of UI trigger buttons for a more professional demo.

Usage:
    python test_threats.py                    # Interactive menu
    python test_threats.py --scenario bot     # Run specific scenario
    python test_threats.py --random 5         # Generate 5 random threats
    python test_threats.py --stress 20        # Stress test with 20 threats
"""

import argparse
import asyncio
import httpx
import random
import sys
from datetime import datetime
from typing import Optional

# Configuration
API_BASE_URL = "http://localhost:8000"

# Threat scenarios for testing
THREAT_SCENARIOS = {
    "bot": {
        "name": "Bot Traffic Attack",
        "threat_type": "bot_traffic",
        "customer_name": "Acme Corp",
        "source_ip": "185.220.101.42",
        "user_agent": "python-requests/2.28.0",
        "request_count": 15000,
        "time_window_minutes": 5,
        "description": "High-volume bot traffic from known malicious IP"
    },
    "credential": {
        "name": "Credential Stuffing",
        "threat_type": "credential_stuffing",
        "customer_name": "FinanceHub",
        "source_ip": "91.134.152.78",
        "user_agent": "Mozilla/5.0 (compatible; Googlebot/2.1)",
        "request_count": 5000,
        "time_window_minutes": 10,
        "raw_data": {"failed_logins": 4850, "unique_usernames": 4200},
        "description": "Mass login attempts with leaked credentials"
    },
    "ato": {
        "name": "Account Takeover",
        "threat_type": "account_takeover",
        "customer_name": "CryptoExchange",
        "source_ip": "45.33.32.156",
        "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "request_count": 50,
        "time_window_minutes": 2,
        "raw_data": {"user_id": "user_8847291", "password_changed": True, "mfa_disabled": True},
        "description": "Suspicious account changes after credential compromise"
    },
    "scraping": {
        "name": "Data Scraping",
        "threat_type": "data_scraping",
        "customer_name": "E-Commerce Plus",
        "source_ip": "103.21.244.15",
        "user_agent": "curl/7.68.0",
        "request_count": 25000,
        "time_window_minutes": 30,
        "raw_data": {"endpoints_hit": ["/api/products", "/api/prices", "/api/inventory"]},
        "description": "Systematic scraping of product catalog"
    },
    "geo": {
        "name": "Geographic Anomaly",
        "threat_type": "geo_anomaly",
        "customer_name": "GlobalBank",
        "source_ip": "176.119.147.225",
        "user_agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0)",
        "request_count": 25,
        "time_window_minutes": 5,
        "raw_data": {"usual_location": "New York, US", "current_location": "Lagos, Nigeria", "travel_time_hours": 2},
        "description": "Impossible travel - user login from distant location"
    },
    "rate": {
        "name": "Rate Limit Breach",
        "threat_type": "rate_limit_breach",
        "customer_name": "APIFirst",
        "source_ip": "52.14.85.125",
        "user_agent": "axios/0.27.2",
        "request_count": 50000,
        "time_window_minutes": 1,
        "raw_data": {"api_key": "ak_prod_****7291", "endpoint": "/api/v2/search"},
        "description": "API key exceeding rate limits by 50x"
    },
    "brute": {
        "name": "Brute Force Attack",
        "threat_type": "brute_force",
        "customer_name": "SecurePortal",
        "source_ip": "178.62.232.84",
        "user_agent": "Mozilla/5.0 (Windows NT 6.1)",
        "request_count": 10000,
        "time_window_minutes": 15,
        "raw_data": {"target_account": "admin", "failed_attempts": 9987},
        "description": "Dictionary attack against admin account"
    },
    "benign": {
        "name": "Likely False Positive (Googlebot)",
        "threat_type": "bot_traffic",
        "customer_name": "TechStartup",
        "source_ip": "66.249.66.1",
        "user_agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
        "request_count": 500,
        "time_window_minutes": 60,
        "description": "Legitimate search engine crawler - should score high FP"
    },
}

# Customer names for random generation
CUSTOMERS = [
    "Acme Corp", "TechGiant", "FinanceHub", "CryptoExchange", 
    "E-Commerce Plus", "GlobalBank", "SecurePortal", "CloudServices",
    "DataDriven", "APIFirst", "StartupXYZ", "EnterpriseCo"
]


def print_banner():
    """Print script banner."""
    print("\n" + "=" * 60)
    print("  SOC Agent System - Threat Test Script")
    print("=" * 60)
    print(f"  Target: {API_BASE_URL}")
    print(f"  Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60 + "\n")


def print_scenario_menu():
    """Print available scenarios."""
    print("Available Test Scenarios:\n")
    for key, scenario in THREAT_SCENARIOS.items():
        print(f"  [{key:10}] {scenario['name']}")
        print(f"              {scenario['description']}")
        print()


async def trigger_threat(scenario_data: dict) -> dict:
    """Send threat to the API."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{API_BASE_URL}/api/threats/trigger",
            json=scenario_data,
            timeout=30.0
        )
        return response.json()


async def run_scenario(scenario_key: str):
    """Run a specific threat scenario."""
    if scenario_key not in THREAT_SCENARIOS:
        print(f"âŒ Unknown scenario: {scenario_key}")
        print(f"   Available: {', '.join(THREAT_SCENARIOS.keys())}")
        return

    scenario = THREAT_SCENARIOS[scenario_key]
    print(f"\nðŸš€ Running Scenario: {scenario['name']}")
    print(f"   Type: {scenario['threat_type']}")
    print(f"   Customer: {scenario['customer_name']}")
    print(f"   Source IP: {scenario['source_ip']}")
    print("-" * 40)

    try:
        result = await trigger_threat({
            "threat_type": scenario["threat_type"],
            "customer_name": scenario["customer_name"],
            "source_ip": scenario["source_ip"],
            "user_agent": scenario.get("user_agent"),
            "request_count": scenario.get("request_count", 1000),
            "time_window_minutes": scenario.get("time_window_minutes", 5),
            "raw_data": scenario.get("raw_data", {}),
        })

        print(f"\nâœ… Threat Generated Successfully")
        print(f"   ID: {result.get('id', 'N/A')}")
        print(f"   Severity: {result.get('severity', 'N/A')}")
        if result.get('false_positive_score'):
            fp = result['false_positive_score']
            print(f"   FP Score: {fp.get('score', 'N/A'):.2f} ({fp.get('recommendation', 'N/A')})")
        if result.get('response_plan'):
            rp = result['response_plan']
            print(f"   Action: {rp.get('primary_action', {}).get('action_type', 'N/A')}")
        print(f"   Human Review: {result.get('requires_human_review', False)}")

    except httpx.ConnectError:
        print(f"\nâŒ Connection Error: Cannot reach {API_BASE_URL}")
        print("   Make sure the backend is running: python main.py")
    except Exception as e:
        print(f"\nâŒ Error: {str(e)}")


async def run_random_threats(count: int, delay: float = 1.0):
    """Generate random threats."""
    print(f"\nðŸŽ² Generating {count} Random Threats")
    print("-" * 40)

    threat_types = list(THREAT_SCENARIOS.keys())

    for i in range(count):
        scenario_key = random.choice(threat_types)
        scenario = THREAT_SCENARIOS[scenario_key].copy()

        # Randomize customer
        scenario["customer_name"] = random.choice(CUSTOMERS)

        # Randomize IP
        scenario["source_ip"] = f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"

        print(f"\n[{i+1}/{count}] {scenario['name']} â†’ {scenario['customer_name']}")

        try:
            result = await trigger_threat({
                "threat_type": scenario["threat_type"],
                "customer_name": scenario["customer_name"],
                "source_ip": scenario["source_ip"],
                "user_agent": scenario.get("user_agent"),
                "request_count": scenario.get("request_count", 1000),
                "time_window_minutes": scenario.get("time_window_minutes", 5),
            })

            severity = result.get('severity', 'unknown')
            fp_score = result.get('false_positive_score', {}).get('score', 0)
            print(f"        âœ“ Severity: {severity}, FP: {fp_score:.2f}")

        except Exception as e:
            print(f"        âœ— Error: {str(e)}")

        if i < count - 1:
            await asyncio.sleep(delay)

    print(f"\nâœ… Generated {count} threats")


async def run_stress_test(count: int):
    """Run stress test with parallel threats."""
    print(f"\nâš¡ Stress Test: {count} Parallel Threats")
    print("-" * 40)

    threat_types = list(THREAT_SCENARIOS.keys())

    async def generate_one(index: int):
        scenario_key = random.choice(threat_types)
        scenario = THREAT_SCENARIOS[scenario_key].copy()
        scenario["customer_name"] = random.choice(CUSTOMERS)
        scenario["source_ip"] = f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"

        try:
            result = await trigger_threat({
                "threat_type": scenario["threat_type"],
                "customer_name": scenario["customer_name"],
                "source_ip": scenario["source_ip"],
                "request_count": scenario.get("request_count", 1000),
            })
            return ("success", result.get('severity', 'unknown'))
        except Exception as e:
            return ("error", str(e))

    start_time = datetime.now()
    tasks = [generate_one(i) for i in range(count)]
    results = await asyncio.gather(*tasks)
    elapsed = (datetime.now() - start_time).total_seconds()

    successes = sum(1 for r in results if r[0] == "success")
    errors = count - successes

    print(f"\nâœ… Stress Test Complete")
    print(f"   Total: {count}")
    print(f"   Success: {successes}")
    print(f"   Errors: {errors}")
    print(f"   Time: {elapsed:.2f}s")
    print(f"   Rate: {count/elapsed:.1f} threats/second")


async def interactive_menu():
    """Run interactive menu."""
    while True:
        print("\n" + "-" * 40)
        print("Options:")
        print("  1-8  Run specific scenario")
        print("  r    Random threat")
        print("  s    Stress test (10 parallel)")
        print("  l    List scenarios")
        print("  q    Quit")
        print("-" * 40)

        choice = input("\nChoice: ").strip().lower()

        scenario_keys = list(THREAT_SCENARIOS.keys())

        if choice == 'q':
            print("\nGoodbye! ðŸ‘‹")
            break
        elif choice == 'l':
            print_scenario_menu()
        elif choice == 'r':
            await run_random_threats(1, 0)
        elif choice == 's':
            await run_stress_test(10)
        elif choice in scenario_keys:
            await run_scenario(choice)
        elif choice.isdigit() and 1 <= int(choice) <= len(scenario_keys):
            await run_scenario(scenario_keys[int(choice) - 1])
        else:
            # Try to match partial scenario name
            matches = [k for k in scenario_keys if k.startswith(choice)]
            if len(matches) == 1:
                await run_scenario(matches[0])
            else:
                print(f"Unknown option: {choice}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="SOC Agent System - Threat Test Script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python test_threats.py                  # Interactive menu
  python test_threats.py --scenario bot   # Run bot attack scenario
  python test_threats.py --random 5       # Generate 5 random threats
  python test_threats.py --stress 20      # Stress test with 20 threats
  python test_threats.py --list           # List all scenarios
        """
    )

    parser.add_argument('--scenario', '-s', type=str, help='Run specific scenario')
    parser.add_argument('--random', '-r', type=int, help='Generate N random threats')
    parser.add_argument('--stress', type=int, help='Stress test with N parallel threats')
    parser.add_argument('--list', '-l', action='store_true', help='List available scenarios')
    parser.add_argument('--url', type=str, default=API_BASE_URL, help='API base URL')

    args = parser.parse_args()

    global API_BASE_URL
    API_BASE_URL = args.url

    print_banner()

    if args.list:
        print_scenario_menu()
    elif args.scenario:
        asyncio.run(run_scenario(args.scenario))
    elif args.random:
        asyncio.run(run_random_threats(args.random))
    elif args.stress:
        asyncio.run(run_stress_test(args.stress))
    else:
        asyncio.run(interactive_menu())


if __name__ == "__main__":
    main()
```

**Usage Examples:**

```bash
# Interactive menu
python test_threats.py

# Run specific scenario
python test_threats.py --scenario bot
python test_threats.py -s credential
python test_threats.py -s benign  # Test FP scoring

# Random threats
python test_threats.py --random 5

# Stress test
python test_threats.py --stress 20

# List scenarios
python test_threats.py --list
```

---

## Updated API Endpoints

**File:** `backend/src/main.py`

Add these endpoints to support filtering and the test script:

```python
# Add to imports
from models import ThreatFilter

# Add this endpoint for filtered threat queries
@app.post("/api/threats/filter")
async def filter_threats(filters: ThreatFilter) -> List[ThreatAnalysis]:
    """Filter threats based on criteria."""
    filtered = analysis_store

    if filters.severities:
        filtered = [t for t in filtered if t.severity in filters.severities]

    if filters.statuses:
        filtered = [t for t in filtered if t.status in filters.statuses]

    if filters.requires_human_review is not None:
        filtered = [t for t in filtered if t.requires_human_review == filters.requires_human_review]

    if filters.search_query:
        query = filters.search_query.lower()
        filtered = [t for t in filtered if 
            query in t.signal.customer_name.lower() or
            query in t.signal.threat_type.value.lower() or
            query in t.executive_summary.lower()
        ]

    return filtered


# Update metrics endpoint to include review count
@app.get("/api/metrics")
async def get_metrics() -> DashboardMetrics:
    """Get dashboard metrics."""
    threats = analysis_store

    return DashboardMetrics(
        total_threats=len(threats),
        threats_by_severity={
            s.value: sum(1 for t in threats if t.severity == s)
            for s in ThreatSeverity
        },
        threats_by_status={
            s.value: sum(1 for t in threats if t.status == s)
            for s in ThreatStatus
        },
        pending_human_review=sum(1 for t in threats if t.requires_human_review),
        avg_processing_time_ms=sum(t.total_processing_time_ms for t in threats) / max(len(threats), 1),
        # ... other metrics
    )
```

---

## Remove UI Trigger Buttons

**File:** `frontend/src/components/Header.jsx` or wherever triggers exist

Remove or comment out the trigger buttons:

```jsx
// REMOVE THIS SECTION:
// <div className="flex gap-2">
//   <button onClick={() => triggerThreat('bot_traffic')} className="...">
//     Trigger Bot Attack
//   </button>
//   <button onClick={() => triggerThreat('random')} className="...">
//     Trigger Random
//   </button>
// </div>

// Replace with connection status only:
<div className="flex items-center gap-2">
  <span className={`w-2 h-2 rounded-full ${isConnected ? 'bg-green-500' : 'bg-red-500'}`} />
  <span className="text-gray-400 text-sm">
    {isConnected ? 'Live' : 'Disconnected'}
  </span>
</div>
```

---

## Demo Strategy

### 5-Minute Demo Flow

1. **Setup (30 sec)**
   - Show two terminals: Backend logs + Test script
   - Browser with dashboard open

2. **Single Threat Demo (1 min)**
   ```bash
   python test_threats.py --scenario credential
   ```
   - Watch logs show parallel agent execution
   - Point out FP score, response actions, timeline

3. **False Positive Demo (1 min)**
   ```bash
   python test_threats.py --scenario benign
   ```
   - Show how Googlebot traffic scores high FP
   - Explain ML-based FP detection

4. **Filtering Demo (1 min)**
   - Click "Needs Review" filter
   - Show severity filters
   - Search for customer name

5. **Scale Demo (1 min)**
   ```bash
   python test_threats.py --stress 10
   ```
   - Show parallel processing handles load
   - Point out consistent < 2s response times

6. **Q&A / Deep Dive (30 sec)**
   - Click into threat detail
   - Show Investigation Timeline tab
   - Show Response Actions tab

### Key Talking Points

**For Technical Interviewers:**
- "Multi-agent architecture enables specialization and parallel execution"
- "FP scoring uses historical patterns and rule-based indicators"
- "Response engine maps threat+severity to playbook actions"
- "Timeline provides audit trail for compliance"

**For Product/Business:**
- "Filters help analysts focus on what needs attention"
- "FP scoring reduces alert fatigue by 40%+"
- "Automated response recommendations speed up MTTR"
- "Customer narrative enables clear communication"

---

## File Structure Summary

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ coordinator.py      # UPDATED - Enhanced orchestration
â”‚   â”‚   â”œâ”€â”€ base_agent.py       # Existing
â”‚   â”‚   â””â”€â”€ ...                 # Other agents
â”‚   â”œâ”€â”€ analyzers/              # NEW DIRECTORY
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fp_analyzer.py      # NEW - False positive scoring
â”‚   â”‚   â”œâ”€â”€ response_engine.py  # NEW - Response actions
â”‚   â”‚   â””â”€â”€ timeline_builder.py # NEW - Investigation timeline
â”‚   â”œâ”€â”€ models.py               # UPDATED - New data models
â”‚   â”œâ”€â”€ main.py                 # UPDATED - New endpoints
â”‚   â””â”€â”€ ...
â”œâ”€â”€ test_threats.py             # NEW - Test script
â””â”€â”€ ...

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ThreatFilters.jsx   # NEW - Filtering UI
â”‚   â”‚   â”œâ”€â”€ ThreatList.jsx      # UPDATED - Uses filters
â”‚   â”‚   â”œâ”€â”€ ThreatDetail.jsx    # UPDATED - Tabs for new data
â”‚   â”‚   â””â”€â”€ Header.jsx          # UPDATED - Remove triggers
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

---

## Quick Start

```bash
# 1. Update backend files with new code above

# 2. Create analyzers directory
mkdir -p backend/src/analyzers
touch backend/src/analyzers/__init__.py

# 3. Start backend
cd backend/src
python main.py

# 4. Start frontend (separate terminal)
cd frontend
npm run dev

# 5. Generate test threats (separate terminal)
cd backend
python test_threats.py --scenario bot
python test_threats.py --random 5
```

---

**Your SOC system now looks production-grade!** ðŸš€
